{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "SEED = 42\n",
    "\n",
    "\n",
    "\n",
    "from utils import *\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%pip install nltk"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:52:10.279498Z",
     "start_time": "2024-03-14T06:52:10.269071Z"
    }
   },
   "source": [
    "vocab_kos = load_vocabulary(\"data/bag+of+words/vocab.kos.txt\")\n",
    "vocab_nips = load_vocabulary(\"data/bag+of+words/vocab.nips.txt\")\n",
    "display(len(vocab_kos))\n",
    "display(len(vocab_nips))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:52:10.903648Z",
     "start_time": "2024-03-14T06:52:10.371697Z"
    }
   },
   "source": [
    "counts_matrix_kos = load_docwords(\"data/bag+of+words/docword.kos.txt\")\n",
    "counts_matrix_nips = load_docwords(\"data/bag+of+words/docword.nips.txt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:52:10.909939Z",
     "start_time": "2024-03-14T06:52:10.905126Z"
    }
   },
   "source": [
    "counts_matrix_nips"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:52:10.915077Z",
     "start_time": "2024-03-14T06:52:10.911355Z"
    }
   },
   "source": [
    "tfdidf_transformer = TfidfTransformer()\n",
    "tf_idf_matrix_kos = tfdidf_transformer.fit_transform(counts_matrix_kos)\n",
    "tf_idf_matrix_nips = tfdidf_transformer.fit_transform(counts_matrix_nips)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Lemmatization - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_vocabulary = []\n",
    "\n",
    "for word in vocab_nips:\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    # if lemmatized_word != word:\n",
    "    #     print(word, lemmatized_word)\n",
    "    lemmatized_vocabulary.append(lemmatized_word)\n",
    "\n",
    "print(len(lemmatized_vocabulary), len(set(lemmatized_vocabulary))) #1000 words less -> lower dimension -> better dimred??\n",
    "print(lemmatized_vocabulary)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "mapping = {}\n",
    "lemma_kos = list(np.unique(lemmatized_vocabulary))\n",
    "print(len(lemma_kos))\n",
    "for i, word in enumerate(vocab_kos):\n",
    "    idx_in_new = lemma_kos.index(lemmatizer.lemmatize(word))\n",
    "    mapping[i] = idx_in_new\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "with open(\"data/bag+of+words/docword.kos.txt\", \"r\") as f:\n",
    "    n_docs = int(f.readline().strip())\n",
    "    n_words = int(f.readline().strip())\n",
    "    n_words = len(lemma_kos)\n",
    "    n_nonzero_counts = int(f.readline().strip())\n",
    "    print(n_docs, n_words, n_nonzero_counts)\n",
    "    counts_lemma_kos = np.zeros((n_docs, n_words))\n",
    "    for line in f:\n",
    "        split_line = line.strip().split(\" \")\n",
    "        word_id = int(split_line[1])-1\n",
    "        new_word_id = mapping[word_id]\n",
    "        counts_lemma_kos[int(split_line[0])-1, new_word_id] += int(split_line[2])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "source": [
    "counts_lemma_kos"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "tf_idf_matrix_kos_lemma = tfdidf_transformer.fit_transform(counts_lemma_kos)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "%pip install umap-learn"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:52:11.508555Z",
     "start_time": "2024-03-14T06:52:11.504999Z"
    }
   },
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "#umap\n",
    "import umap\n",
    "#lsp\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:52:11.515071Z",
     "start_time": "2024-03-14T06:52:11.511150Z"
    }
   },
   "source": [
    "def reduce_dim(reducer, data, colour=None, cmap=None, verbose=False):\n",
    "    reduced_matrix = reducer.fit_transform(data)\n",
    "    if verbose:\n",
    "        display(reduced_matrix)\n",
    "\n",
    "    scatter = plt.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], c=colour, cmap=cmap, s=2)\n",
    "    if colour is not None:\n",
    "        plt.legend(*scatter.legend_elements())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:52:11.665977Z",
     "start_time": "2024-03-14T06:52:11.518313Z"
    }
   },
   "source": [
    "reduce_dim(PCA(n_components=2, svd_solver=\"arpack\"), tf_idf_matrix_kos)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "reduce_dim(PCA(n_components=2, svd_solver=\"arpack\"), tf_idf_matrix_kos_lemma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "reduce_dim(PCA(n_components=2, svd_solver=\"arpack\"), tf_idf_matrix_nips)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:53:39.288726Z",
     "start_time": "2024-03-14T06:53:28.559683Z"
    }
   },
   "source": [
    "reduce_dim(TSNE(n_components=2, init=\"random\", random_state=SEED), tf_idf_matrix_kos)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "reduce_dim(TSNE(n_components=2, init=\"random\", random_state=SEED), tf_idf_matrix_kos_lemma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "reduce_dim(TSNE(n_components=2, init=\"random\", random_state=SEED), tf_idf_matrix_nips)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "#many hyperparams!\n",
    "reduce_dim(umap.UMAP(n_components=2), tf_idf_matrix_kos)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "reduce_dim(umap.UMAP(n_components=2), tf_idf_matrix_kos_lemma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "reduce_dim(umap.UMAP(n_components=2), tf_idf_matrix_nips)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not working (https://github.com/hhliz/LSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "source": [
    "def get_words(topic_words_matrix, vocabulary, n=5):\n",
    "    for i in range(topic_words_matrix.shape[0]):\n",
    "        ind =  np.argpartition(topic_words_matrix[i, :], -n)[-n:]\n",
    "\n",
    "        words = [vocabulary[word_id] for word_id in ind]\n",
    "        print(i, words) "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_kos = LatentDirichletAllocation(n_components=10, random_state=SEED)\n",
    "\n",
    "X_new_kos = lda_kos.fit_transform(counts_matrix_kos)\n",
    "lda_nips = LatentDirichletAllocation(n_components=10, random_state=SEED)\n",
    "X_new_nips = lda_nips.fit_transform(counts_matrix_nips)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "themes_kos = np.argmax(X_new_kos, axis=1)\n",
    "themes_nips = np.argmax(X_new_nips, axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "source": [
    "get_words(lda_kos.components_, vocab_kos, n=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "source": [
    "get_words(lda_nips.components_, vocab_nips, n=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "reduce_dim(TSNE(n_components=2, init=\"random\", random_state=SEED), tf_idf_matrix_kos, colour=themes_kos, cmap=\"tab20b\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "reduce_dim(TSNE(n_components=2, init=\"random\", random_state=SEED), tf_idf_matrix_nips, colour=themes_nips, cmap=\"tab20b\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import NMF, MiniBatchNMF"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "source": [
    "nmf_kos = NMF(n_components=10, random_state=SEED)\n",
    "nmf_nips = NMF(n_components=10, random_state=SEED)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "W_kos = nmf_kos.fit_transform(tf_idf_matrix_kos) #documents # topics\n",
    "H_kos = nmf_kos.components_ #topics x words\n",
    "W_nips = nmf_nips.fit_transform(tf_idf_matrix_nips) #documents # topics\n",
    "H_nips = nmf_nips.components_ #topics x words"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "topics_nmf_kos = np.argmax(W_kos, axis=1)\n",
    "topics_nmf_nips = np.argmax(W_nips, axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "source": [
    "reduce_dim(TSNE(n_components=2, init=\"random\", random_state=SEED), tf_idf_matrix_kos, colour=topics_nmf_kos, cmap=\"tab20b\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "source": [
    "reduce_dim(TSNE(n_components=2, init=\"random\", random_state=SEED), tf_idf_matrix_nips, colour=topics_nmf_nips, cmap=\"tab20b\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "source": [
    "get_words(H_kos, vocabulary=vocab_kos, n=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "source": [
    "get_words(H_nips, vocabulary=vocab_nips, n=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "source": [
    "themes_kos"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "np.array(tf_idf_matrix_kos.todense())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "source": [
    "max_kos = np.argmax(np.array(tf_idf_matrix_kos.todense()), axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "source": [
    "np.unique(max_kos).shape"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
